---
title: "Sentiment"
author: "Mina Narayanan"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
## Global knitr options
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(autodep = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

```

```{r, warning = FALSE, message = FALSE}
## Increase Memory Limit
invisible(utils::memory.limit(64000))
options(java.parameters = "-Xmx64000m")

## Data Frame Packages
library(dplyr)
library(stringr)
library(readr)
library(readxl)

## Data Visualization Packages
library(ggplot2)
library(Rtsne)
library(jsonlite)

## Text Mining Pacakges
library(data.table)
library(Matrix)
library(text2vec)
library(tm)
library(SnowballC)
library(rARPACK)
library(ggupset)

# Sentiment Packages
library(tidytext)
library(tidyverse)
library(textdata)
library(stringr)
library(janeaustenr)
library(wordcloud)
```

## Load Data

First we load the data of NINR grants awarded in 2019.  We pull four sources of text:
\begin{itemize}
\item T - Title
\item A - Abstract Text
\item S - Specific Aims (SA) Text
\end{itemize}x
```{r, message = FALSE, cache = TRUE}
ht_1 = read_csv("../ht/ht1-6monthsscrapedinfo.csv", guess_max = 1000000)
ht_2 = read_csv("../ht/ht7-12monthsscrapedinfo.csv", guess_max = 1000000)

ht <- rbind(ht_1, ht_2)
rm(ht_1)
rm(ht_2)
```

### Clean Up Text

Let's clean up the text by making everything lowercase, removing punctuation, etc.
```{r}
# Remove weird characters
ht$article_text = str_replace_all(ht$article_text, "\n", " ")
ht$article_text = str_replace_all(ht$article_text, "\r", " ")
ht$article_text = str_replace_all(ht$article_text, "'", "")
ht$article_text = str_replace_all(ht$article_text, "-", " ")

# Replace non-alpha numeric characters with a space
ht$article_text = str_replace_all(ht$article_text, "[^abcdefghijklmnopqrstuvwxyzABCDEFHIJKLMNOPQRSTUVWXZ ]", " ")

# Put everything in lowercase
ht$article_text = tolower(ht$article_text)

# Remove extra spaces and strip whitespace padding
ht$article_text = str_replace_all(ht$article_text, "\\s+", " ")
ht$article_text = str_trim(ht$article_text, side = "both")

```

### Unique Text

Let's only keep applications with unique text.
```{r}
ht <- ht[!duplicated(ht[ , c("article_text")]),]
nrow(ht)
ht$id <- factor(1:nrow(ht))

```
### Remove unnecessary columns and tokenize
```{r}

ht_slim <- ht %>% select(url, title, article_text)

ht_tokens <- ht_slim %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word, article_text)

```

### Compare three sentiment dictionaries
```{r}
afinn <- ht_tokens %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(ht_tokens %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          ht_tokens %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
### Bing Sentiment Analysis with Inner Join
```{r}
ht_bing_sentiment_index <- ht_tokens %>%
  inner_join(get_sentiments("bing"))%>%
  count(index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ht_bing_sentiment <- ht_tokens %>%
  inner_join(get_sentiments("bing"))

ggplot(ht_bing_sentiment_index, aes(index, sentiment)) +
  ylim(-100, 100) + 
  geom_col(show.legend = FALSE) 
  #facet_wrap(~book, ncol = 2, scales = "free_x")

ht_bing_sentiment_index <- ht_bing_sentiment_index %>%
                            rename(linenumber = index)

ht_bing <- merge(x = ht_bing_sentiment, y = ht_bing_sentiment_index, by = "linenumber", all.x = TRUE)
ht_bing <- ht_bing[!duplicated(ht_bing[ , c("linenumber")]),]
drops = c("sentiment.x", "word")
ht_bing <- ht_bing[, !(names(ht_bing) %in% drops)]
ht_bing <- ht_bing %>% rename(sentiment = sentiment.y)

write_csv(ht_bing, "sentiment_scores_bing.csv")

```
### NRC Sentiment Analysis with Inner Join
```{r}

ht_nrc_sentiment_index <- ht_tokens %>%
  inner_join(get_sentiments("nrc"))%>%
  count(index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ht_nrc_sentiment <- ht_tokens %>%
  inner_join(get_sentiments("nrc"))

ggplot(ht_nrc_sentiment_index, aes(index, sentiment)) +
  ylim(-125, 250) + 
  geom_col(show.legend = FALSE) 
  #facet_wrap(~book, ncol = 2, scales = "free_x")

ht_nrc_sentiment_index <- ht_nrc_sentiment_index %>%
                            rename(linenumber = index)

ht_nrc <- merge(x = ht_nrc_sentiment, y = ht_nrc_sentiment_index, by = "linenumber", all.x = TRUE)
ht_nrc <- ht_nrc[!duplicated(ht_nrc[ , c("linenumber")]),]
drops = c("sentiment.x", "word")
ht_nrc <- ht_nrc[, !(names(ht_nrc) %in% drops)]
ht_nrc <- ht_nrc %>% rename(sentiment = sentiment.y)

write_csv(ht_nrc, "sentiment_scores_nrc.csv")
```


### AFINN Seniment Analysis with Inner Join
```{r}

ht_afinn_sentiment <- ht_tokens %>%
  inner_join(get_sentiments("afinn"))

ht_afinn_sentiment_index <- ht_afinn_sentiment %>%
  group_by(index = linenumber) %>%
  summarise(negative = sum(value < 0), positive = sum(value > 0), sentiment = positive - negative ) %>% 
  mutate(method = "AFINN")

ggplot(ht_afinn_sentiment_index, aes(index, sentiment)) +
  ylim(-100, 100) + 
  geom_col(show.legend = FALSE) 
  #facet_wrap(~book, ncol = 2, scales = "free_x")

ht_afinn_sentiment_index <- ht_afinn_sentiment_index %>%
                            rename(linenumber = index)

ht_afinn <- merge(x = ht_afinn_sentiment, y = ht_afinn_sentiment_index, by = "linenumber", all.x = TRUE)
ht_afinn <- ht_afinn[!duplicated(ht_afinn[ , c("linenumber")]),]
drops = c("value", "word", "method")
ht_afinn <- ht_afinn[, !(names(ht_afinn) %in% drops)]

write_csv(ht_afinn, "sentiment_scores_afinn.csv")

```


### Word Clouds
```{r}
ht_tokens %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

